{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fd2681-b215-461d-affb-a582e2409f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import gc\n",
    "\n",
    "# ---------------------------- Reset Keras Session ---------------------------- #\n",
    "def reset_keras():\n",
    "    from tensorflow.keras.backend import clear_session\n",
    "    clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "# ---------------------------- ROC Curve Plotting ---------------------------- #\n",
    "def roc(y_tests, y_test_scores, save_path=None):\n",
    "    font = {'family': 'arial',\n",
    "            'weight': 'bold',\n",
    "            'size': 20}\n",
    "    params = {'axes.labelsize': '20',\n",
    "              'xtick.labelsize': '20',\n",
    "              'ytick.labelsize': '20',\n",
    "              'lines.linewidth': '4'}\n",
    "    pylab.rcParams.update(params)\n",
    "    pylab.rcParams['font.family'] = 'sans-serif'\n",
    "    pylab.rcParams['font.sans-serif'] = ['Arial']\n",
    "    pylab.rcParams['font.weight'] = 'bold'\n",
    "    plt.figure(figsize=(7, 7), dpi=300)\n",
    "    AUC = roc_auc_score(y_tests, y_test_scores)\n",
    "    fpr1, tpr1, thresholds1 = roc_curve(y_tests, y_test_scores)\n",
    "    plt.plot(fpr1, tpr1, linewidth=3, color='tomato', label='AUC = {:.3f}'.format(AUC))\n",
    "    plt.plot([0, 1], [0, 1], linewidth=1, color='grey', linestyle=\"--\")\n",
    "    plt.yticks(np.linspace(0, 1, 6))\n",
    "    plt.xticks(np.linspace(0, 1, 6))\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((0, 1))\n",
    "    plt.legend(prop={'size': 20}, loc=4, frameon=False)\n",
    "    plt.subplots_adjust(left=0.2, right=0.95, top=0.95, bottom=0.2)\n",
    "    plt.xlabel('1–Specificity', font)\n",
    "    plt.ylabel('Sensitivity', font)\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------- Custom Layers ---------------------------- #\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)      # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(query, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(key, batch_size)      # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(value, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(concat_attention)  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.005):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "# ---------------------------- Helper Functions ---------------------------- #\n",
    "def seq2num(seqlist, maxlen=21):\n",
    "    out = []\n",
    "    transdic = {\n",
    "        'A': 8, 'C': 1, 'D': 2, 'E': 3, 'F': 4, 'G': 5, 'H': 6,\n",
    "        'I': 7, 'K': 0, 'L': 9, 'M': 10, 'N': 11, 'P': 12,\n",
    "        'Q': 13, 'R': 14, 'S': 15, 'T': 16, 'V': 17, 'W': 18,\n",
    "        'Y': 19, '*': 20\n",
    "    }\n",
    "    for seq in seqlist:\n",
    "        seq = seq.replace('U', '*').replace('X', '*')\n",
    "        vec = [transdic.get(i, 20) for i in seq]  # 使用 get 方法处理未定义的字符\n",
    "        # 确保序列长度为maxlen\n",
    "        if len(vec) < maxlen:\n",
    "            vec += [20] * (maxlen - len(vec))  # 填充\n",
    "        else:\n",
    "            vec = vec[:maxlen]  # 截断\n",
    "        out.append(vec)\n",
    "    out = np.array(out)\n",
    "    return out\n",
    "\n",
    "def turn_to_float64(feature):\n",
    "    x = np.array(feature, dtype=np.float64)\n",
    "    y = x.tolist()\n",
    "    return y\n",
    "\n",
    "# ---------------------------- Model Building Functions ---------------------------- #\n",
    "def build_transformer_encoder(maxlen, vocab_size, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "    inputs = layers.Input(shape=(maxlen,), name=\"Sequence_Input\")\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim, rate)\n",
    "    x = transformer_block(x, training=False)  # 设置为推理模式\n",
    "    x = transformer_block(x, training=False)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(0.05)(x)\n",
    "    encoder = Model(inputs=inputs, outputs=x, name=\"Transformer_Encoder\")\n",
    "    return encoder\n",
    "\n",
    "def build_autoencoder(encoder, vocab_size, maxlen):\n",
    "    # Decoder部分，用于重建输入序列\n",
    "    # 使用一个 Dense 层将编码器输出映射到 maxlen * vocab_size\n",
    "    decoder = layers.Dense(maxlen * vocab_size, activation='softmax')(encoder.output)\n",
    "    # 重新塑形为 (batch_size, maxlen, vocab_size)\n",
    "    decoder = layers.Reshape((maxlen, vocab_size))(decoder)\n",
    "    autoencoder = Model(inputs=encoder.input, outputs=decoder, name=\"Transformer_Autoencoder\")\n",
    "    return autoencoder\n",
    "\n",
    "def build_dnn_classifier(input_dim):\n",
    "    inputs = layers.Input(shape=(input_dim,), name=\"Combined_Features\")\n",
    "    x = layers.Dense(64, activation='selu')(inputs)\n",
    "    x = layers.Dense(16, activation='selu')(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    outputs = layers.Dense(2, activation='softmax')(x)\n",
    "    classifier = Model(inputs=inputs, outputs=outputs, name=\"DNN_Classifier\")\n",
    "    return classifier\n",
    "\n",
    "# ---------------------------- Main Execution ---------------------------- #\n",
    "if __name__ == '__main__':\n",
    "    '''读取数据并准备'''\n",
    "    vocab_size = 600  # 词汇表大小，根据需要调整\n",
    "    maxlen = 21\n",
    "    embed_dim = 128  # 每个 token 的嵌入维度\n",
    "    num_heads = 4  # 注意力头的数量\n",
    "    ff_dim = 64  # Transformer 内部前馈网络的隐藏层大小\n",
    "    dropout_rate = 0.1  # Dropout比例\n",
    "\n",
    "    # 1. 准备数据\n",
    "    trans_or_not = False  # 根据需要设置为 True 或 False\n",
    "    namelist, data, label = prepare_data1(trans_or_not)\n",
    "    data = seq2num(data, maxlen=maxlen)\n",
    "\n",
    "    # 2. 定义 Transformer 编码器\n",
    "    encoder = build_transformer_encoder(maxlen, vocab_size, embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "    encoder.summary()\n",
    "\n",
    "    # 3. 构建自编码器模型\n",
    "    autoencoder = build_autoencoder(encoder, vocab_size, maxlen)\n",
    "    autoencoder.summary()\n",
    "\n",
    "    # 4. 编译自编码器\n",
    "    autoencoder.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # 5. 准备输入和输出\n",
    "    # 对于自编码任务，输入和输出相同\n",
    "    x = data  # (num_samples, maxlen)\n",
    "    y = data  # (num_samples, maxlen)\n",
    "\n",
    "    # 确保 y 的数据类型为整数类型\n",
    "    y = y.astype(np.int32)\n",
    "\n",
    "    # 6. 训练自编码器\n",
    "    history = autoencoder.fit(\n",
    "        x, y,\n",
    "        batch_size=64,\n",
    "        epochs=50,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "            ModelCheckpoint('transformer_autoencoder_best.model', save_best_only=True)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 7. 保存Transformer编码器\n",
    "    encoder.save('transformer_encoder.model')\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data1(transfer_or_not, maxlen=21):\n",
    "    if transfer_or_not:\n",
    "        path = './transfer'\n",
    "    else:\n",
    "        path = '.'\n",
    "    df = pd.read_csv(os.path.join(path, 'Kcr_label.csv'))\n",
    "    pepname = df['pepname'].values\n",
    "    label = df['label'].values  # 仅用于分离序列，不用于编码\n",
    "\n",
    "    def seq_dic(fileplace):\n",
    "        with open(fileplace, mode='r') as file:\n",
    "            peptides = file.readlines()\n",
    "            pepdict = {}\n",
    "            for peptide in peptides:\n",
    "                peptide = peptide.rstrip().split('\\t')\n",
    "                pepdict[peptide[0]] = peptide[1]\n",
    "        return pepdict\n",
    "\n",
    "    pos_dict = seq_dic(os.path.join(path, 'pos_Kcr.txt'))\n",
    "    neg_dict = seq_dic(os.path.join(path, 'neg_Kcr.txt'))\n",
    "\n",
    "    # 合并正负样本到一个字典中，避免模型通过序列来源推断标签\n",
    "    all_dict = {**pos_dict, **neg_dict}\n",
    "\n",
    "    pep_seq = []\n",
    "    for pepID in pepname:\n",
    "        sequence = all_dict.get(pepID, '')\n",
    "        if sequence:\n",
    "            pep_seq.append(sequence)\n",
    "        else:\n",
    "            # 处理缺失序列的情况，可以选择跳过或填充\n",
    "            pep_seq.append('*' * maxlen)  # 示例：用'*'填充，确保序列长度为21\n",
    "\n",
    "    # 转换为 NumPy 数组并打乱\n",
    "    data = np.array(pep_seq)\n",
    "    labels = np.array(label)\n",
    "    peps = np.array(pepname)\n",
    "    data, labels, peps = shuffle(data, labels, peps, random_state=42)\n",
    "\n",
    "    return peps, data, labels\n",
    "\n",
    "def getfeatures1(namelist, labels, embed_dim=128):\n",
    "    feature = []\n",
    "    for i, name in enumerate(namelist):\n",
    "        if labels[i] == 0:\n",
    "            fileplace = './results/10features_for_negative_data1'\n",
    "        else:\n",
    "            fileplace = './results/10features1'\n",
    "        transformer_path = os.path.join(fileplace, 'transformer', f\"{name}.transformer\")\n",
    "        if os.path.exists(transformer_path):\n",
    "            with open(transformer_path, mode='r') as file:\n",
    "                fea = file.read().rstrip().split('\\t')\n",
    "                fea = turn_to_float64(fea)\n",
    "                feature.append(fea)\n",
    "        else:\n",
    "            # 处理缺失文件的情况\n",
    "            feature.append([0.0] * embed_dim)  # 示例：用0填充\n",
    "    return np.array(feature)\n",
    "\n",
    "def store_code1(peplist, codes, labels, transfer_or_not):\n",
    "    for i, la in enumerate(labels):\n",
    "        if transfer_or_not:\n",
    "            storehouse = './transfer'\n",
    "        else:\n",
    "            storehouse = './results'\n",
    "        if la == 0:\n",
    "            storehouse = os.path.join(storehouse, '10features_for_negative_data1', 'transformer')\n",
    "        else:\n",
    "            storehouse = os.path.join(storehouse, '10features1', 'transformer')\n",
    "        if not os.path.exists(storehouse):\n",
    "            os.makedirs(storehouse)\n",
    "        with open(os.path.join(storehouse, f\"{peplist[i]}.transformer\"), mode='w') as file:\n",
    "            for co in codes[i]:\n",
    "                file.write(str(co) + '\\t')\n",
    "            file.write('\\n')\n",
    "\n",
    "# 提取并保存特征 transfer\n",
    "# encoding transfer-learning data or pre-training data\n",
    "trans_or_not = False\n",
    "namelist, data, label = prepare_data1(trans_or_not)\n",
    "data = seq2num(data, maxlen=maxlen)\n",
    "\n",
    "# 加载训练好的Transformer编码器\n",
    "encoder = load_model('transformer_encoder.model')\n",
    "encoder.summary()\n",
    "\n",
    "# 提取特征\n",
    "encoded_features = encoder.predict(x=data, batch_size=128)\n",
    "\n",
    "# 保存编码后的特征\n",
    "store_code1(namelist, encoded_features, label, trans_or_not)\n",
    "\n",
    "# 提取并保存特征为CSV\n",
    "transformer_feature = getfeatures1(namelist, label, embed_dim=embed_dim)\n",
    "df_trans = pd.DataFrame(transformer_feature)\n",
    "df_peps = pd.DataFrame(namelist, columns=['pepname'])\n",
    "df_labels = pd.DataFrame(label, columns=['label'])\n",
    "df_transformer = pd.concat([df_peps, df_trans, df_labels], axis=1)\n",
    "df_transformer.to_csv('./results/transformer_dataset1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff521e10-5251-49ef-8c28-367ef977e15b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c26f1f-9005-4e9a-8845-ccb0a772f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getfeatures2(namelist, labels, embed_dim=128):\n",
    "    feature = []\n",
    "    for i, name in enumerate(namelist):\n",
    "        if labels[i] == 0:\n",
    "            fileplace = './transfer/10features_for_negative_data1'\n",
    "        else:\n",
    "            fileplace = './transfer/10features1'\n",
    "        transformer_path = os.path.join(fileplace, 'transformer', f\"{name}.transformer\")\n",
    "        if os.path.exists(transformer_path):\n",
    "            with open(transformer_path, mode='r') as file:\n",
    "                fea = file.read().rstrip().split('\\t')\n",
    "                fea = turn_to_float64(fea)\n",
    "                feature.append(fea)\n",
    "        else:\n",
    "            # 处理缺失文件的情况\n",
    "            feature.append([0.0] * embed_dim)  # 示例：用0填充\n",
    "    return np.array(feature)\n",
    "\n",
    "# 提取并保存特征 transfer\n",
    "# encoding transfer-learning data or pre-training data\n",
    "trans_or_not = True\n",
    "namelist, data, label = prepare_data1(trans_or_not)\n",
    "data = seq2num(data, maxlen=maxlen)\n",
    "\n",
    "# 加载训练好的Transformer编码器\n",
    "encoder = load_model('transformer_encoder.model')\n",
    "encoder.summary()\n",
    "\n",
    "# 提取特征\n",
    "encoded_features = encoder.predict(x=data, batch_size=128)\n",
    "\n",
    "# 保存编码后的特征\n",
    "store_code1(namelist, encoded_features, label, trans_or_not)\n",
    "\n",
    "# 提取并保存特征为CSV\n",
    "transformer_feature = getfeatures2(namelist, label, embed_dim=embed_dim)\n",
    "df_trans = pd.DataFrame(transformer_feature)\n",
    "df_peps = pd.DataFrame(namelist, columns=['pepname'])\n",
    "df_labels = pd.DataFrame(label, columns=['label'])\n",
    "df_transformer = pd.concat([df_peps, df_trans, df_labels], axis=1)\n",
    "df_transformer.to_csv('./transfer/transformer_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85893729-c73a-4c2a-bf62-c0971953bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data2():\n",
    "    pep_dict = {}\n",
    "    pep_seq = []\n",
    "\n",
    "    with open('./transfer/experiment_sites.txt', mode='r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            pepname, seq = line.rstrip().split('\\t')\n",
    "            pep_dict[pepname] = seq\n",
    "            pep_seq.append(seq)\n",
    "\n",
    "    return np.array(list(pep_dict.keys())), np.array(pep_seq)\n",
    "\n",
    "def store_code2(peplist, codes):\n",
    "    for i, pepname in enumerate(peplist):\n",
    "        storehouse = './transfer/10features_for_experiment/transformer/'\n",
    "        if not os.path.exists(storehouse):\n",
    "            os.makedirs(storehouse)\n",
    "        with open(storehouse + pepname + r'.transformer', mode='w') as file:\n",
    "            for co in codes[i]:\n",
    "                file.write(str(co) + '\\t')\n",
    "            file.write('\\n')\n",
    "\n",
    "def getfeatures3(namelist):\n",
    "    feature = []\n",
    "    for i, name in enumerate(namelist):\n",
    "        fileplace = './transfer/10features_for_experiment'\n",
    "        transformer_path = os.path.join(fileplace, 'transformer', f\"{name}.transformer\")\n",
    "        if os.path.exists(transformer_path):\n",
    "            with open(transformer_path, mode='r') as file:\n",
    "                fea = file.read().rstrip().split('\\t')\n",
    "                fea = turn_to_float64(fea)\n",
    "                feature.append(fea)\n",
    "        else:\n",
    "            # 处理缺失文件的情况\n",
    "            feature.append([0.0] * embed_dim)  # 示例：用0填充\n",
    "    return np.array(feature)\n",
    "\n",
    "# encoding experiment data for predicting\n",
    "namelist, data = prepare_data2()\n",
    "data = seq2num(data)\n",
    "# 加载训练好的Transformer编码器\n",
    "encoder = load_model('transformer_encoder.model')\n",
    "encoder.summary()\n",
    "# 提取特征\n",
    "encoded_features = encoder.predict(x=data, batch_size=128)\n",
    "# 保存编码后的特征\n",
    "store_code2(namelist, encoded_features)\n",
    "\n",
    "# 提取并保存特征为CSV\n",
    "transformer_feature = getfeatures3(namelist)\n",
    "df_trans = pd.DataFrame(transformer_feature)\n",
    "df_peps = pd.DataFrame(namelist, columns=['pepname'])\n",
    "df_transformer = pd.concat([df_peps, df_trans], axis=1)\n",
    "df_transformer.to_csv('./transfer/10features_for_experiment/transformer_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1ff01e-f87e-4d0c-90ac-785e02d0d1fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d0b47a-bd5a-4ff1-b9a6-e9d8d08f2fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DNN_Classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Combined_Features (InputLa  [(None, 128)]             0         \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 16)                1040      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9330 (36.45 KB)\n",
      "Trainable params: 9330 (36.45 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "---------------- Training Fold 1 -----------------------\n",
      "Epoch 1/50\n",
      "237/256 [==========================>...] - ETA: 0s - loss: 0.9823 - accuracy: 0.5004INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 2s 5ms/step - loss: 0.9747 - accuracy: 0.4996 - val_loss: 0.8180 - val_accuracy: 0.5140\n",
      "Epoch 2/50\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 0.8054 - accuracy: 0.5284INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 1s 3ms/step - loss: 0.8049 - accuracy: 0.5289 - val_loss: 0.7478 - val_accuracy: 0.5388\n",
      "Epoch 3/50\n",
      "249/256 [============================>.] - ETA: 0s - loss: 0.7580 - accuracy: 0.5476INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 1s 3ms/step - loss: 0.7570 - accuracy: 0.5486 - val_loss: 0.7196 - val_accuracy: 0.5648\n",
      "Epoch 4/50\n",
      "232/256 [==========================>...] - ETA: 0s - loss: 0.7300 - accuracy: 0.5649INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 1s 4ms/step - loss: 0.7283 - accuracy: 0.5668 - val_loss: 0.7041 - val_accuracy: 0.5750\n",
      "Epoch 5/50\n",
      "225/256 [=========================>....] - ETA: 0s - loss: 0.7135 - accuracy: 0.5760INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 1s 3ms/step - loss: 0.7133 - accuracy: 0.5777 - val_loss: 0.6924 - val_accuracy: 0.5845\n",
      "Epoch 6/50\n",
      "246/256 [===========================>..] - ETA: 0s - loss: 0.7003 - accuracy: 0.5889INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 1s 3ms/step - loss: 0.7007 - accuracy: 0.5885 - val_loss: 0.6847 - val_accuracy: 0.5954\n",
      "Epoch 7/50\n",
      "256/256 [==============================] - ETA: 0s - loss: 0.6909 - accuracy: 0.6015INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 2s 6ms/step - loss: 0.6909 - accuracy: 0.6015 - val_loss: 0.6780 - val_accuracy: 0.6003\n",
      "Epoch 8/50\n",
      "238/256 [==========================>...] - ETA: 0s - loss: 0.6803 - accuracy: 0.6121INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 1s 3ms/step - loss: 0.6808 - accuracy: 0.6101 - val_loss: 0.6720 - val_accuracy: 0.6038\n",
      "Epoch 9/50\n",
      "220/256 [========================>.....] - ETA: 0s - loss: 0.6757 - accuracy: 0.6156INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 1s 3ms/step - loss: 0.6743 - accuracy: 0.6154 - val_loss: 0.6666 - val_accuracy: 0.6125\n",
      "Epoch 10/50\n",
      "233/256 [==========================>...] - ETA: 0s - loss: 0.6691 - accuracy: 0.6202INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 1s 3ms/step - loss: 0.6685 - accuracy: 0.6201 - val_loss: 0.6625 - val_accuracy: 0.6146\n",
      "Epoch 11/50\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 0.6638 - accuracy: 0.6229INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6236 - val_loss: 0.6589 - val_accuracy: 0.6181\n",
      "Epoch 12/50\n",
      "237/256 [==========================>...] - ETA: 0s - loss: 0.6575 - accuracy: 0.6293INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 1s 5ms/step - loss: 0.6565 - accuracy: 0.6300 - val_loss: 0.6551 - val_accuracy: 0.6210\n",
      "Epoch 13/50\n",
      "245/256 [===========================>..] - ETA: 0s - loss: 0.6511 - accuracy: 0.6378INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 2s 6ms/step - loss: 0.6509 - accuracy: 0.6373 - val_loss: 0.6528 - val_accuracy: 0.6241\n",
      "Epoch 14/50\n",
      "251/256 [============================>.] - ETA: 0s - loss: 0.6492 - accuracy: 0.6409INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 1s 4ms/step - loss: 0.6495 - accuracy: 0.6405 - val_loss: 0.6499 - val_accuracy: 0.6293\n",
      "Epoch 15/50\n",
      "247/256 [===========================>..] - ETA: 0s - loss: 0.6446 - accuracy: 0.6394INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 1s 3ms/step - loss: 0.6445 - accuracy: 0.6391 - val_loss: 0.6480 - val_accuracy: 0.6318\n",
      "Epoch 16/50\n",
      "243/256 [===========================>..] - ETA: 0s - loss: 0.6413 - accuracy: 0.6426INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 1s 3ms/step - loss: 0.6410 - accuracy: 0.6423 - val_loss: 0.6459 - val_accuracy: 0.6327\n",
      "Epoch 17/50\n",
      "240/256 [===========================>..] - ETA: 0s - loss: 0.6375 - accuracy: 0.6454INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 1s 3ms/step - loss: 0.6369 - accuracy: 0.6458 - val_loss: 0.6440 - val_accuracy: 0.6340\n",
      "Epoch 18/50\n",
      "203/256 [======================>.......] - ETA: 0s - loss: 0.6350 - accuracy: 0.6445INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 1s 3ms/step - loss: 0.6338 - accuracy: 0.6462 - val_loss: 0.6421 - val_accuracy: 0.6365\n",
      "Epoch 19/50\n",
      "217/256 [========================>.....] - ETA: 0s - loss: 0.6309 - accuracy: 0.6531INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 1s 3ms/step - loss: 0.6319 - accuracy: 0.6519 - val_loss: 0.6411 - val_accuracy: 0.6386\n",
      "Epoch 20/50\n",
      "238/256 [==========================>...] - ETA: 0s - loss: 0.6269 - accuracy: 0.6540INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 1s 3ms/step - loss: 0.6265 - accuracy: 0.6543 - val_loss: 0.6398 - val_accuracy: 0.6414\n",
      "Epoch 21/50\n",
      "254/256 [============================>.] - ETA: 0s - loss: 0.6252 - accuracy: 0.6581INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6579 - val_loss: 0.6392 - val_accuracy: 0.6398\n",
      "Epoch 22/50\n",
      "244/256 [===========================>..] - ETA: 0s - loss: 0.6272 - accuracy: 0.6527INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6528 - val_loss: 0.6384 - val_accuracy: 0.6402\n",
      "Epoch 23/50\n",
      "228/256 [=========================>....] - ETA: 0s - loss: 0.6216 - accuracy: 0.6617INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 1s 3ms/step - loss: 0.6223 - accuracy: 0.6600 - val_loss: 0.6370 - val_accuracy: 0.6389\n",
      "Epoch 24/50\n",
      "218/256 [========================>.....] - ETA: 0s - loss: 0.6186 - accuracy: 0.6604INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 1s 5ms/step - loss: 0.6187 - accuracy: 0.6600 - val_loss: 0.6363 - val_accuracy: 0.6398\n",
      "Epoch 25/50\n",
      "256/256 [==============================] - ETA: 0s - loss: 0.6182 - accuracy: 0.6604INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 1s 3ms/step - loss: 0.6182 - accuracy: 0.6604 - val_loss: 0.6354 - val_accuracy: 0.6408\n",
      "Epoch 26/50\n",
      "249/256 [============================>.] - ETA: 0s - loss: 0.6174 - accuracy: 0.6633INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_classifier_fold1.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 1s 3ms/step - loss: 0.6172 - accuracy: 0.6633 - val_loss: 0.6347 - val_accuracy: 0.6422\n",
      "Epoch 27/50\n",
      "248/256 [============================>.] - ETA: 0s - loss: 0.6149 - accuracy: 0.6638"
     ]
    }
   ],
   "source": [
    "    # 仅使用Transformer特征进行分类\n",
    "    combined_features = transformer_feature\n",
    "    labels = df_transformer['label'].values\n",
    "\n",
    "    # 定义并编译DNN分类器\n",
    "    dnn = build_dnn_classifier(combined_features.shape[1])\n",
    "    dnn.summary()\n",
    "\n",
    "    dnn.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # 交叉验证\n",
    "    count = 0\n",
    "    sfolder = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    all_loc_pred = []\n",
    "    all_loc_label = []\n",
    "\n",
    "    for train_idx, test_idx in sfolder.split(combined_features, labels):\n",
    "        count += 1\n",
    "        x_train, x_test = combined_features[train_idx], combined_features[test_idx]\n",
    "        y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "\n",
    "        # 转换标签为 one-hot 编码\n",
    "        y_train_cat = to_categorical(y_train)\n",
    "        y_test_cat = to_categorical(y_test)\n",
    "\n",
    "        # 训练分类器\n",
    "        print(f'---------------- Training Fold {count} -----------------------')\n",
    "        dnn.fit(\n",
    "            x_train, y_train_cat,\n",
    "            batch_size=64,\n",
    "            epochs=50,\n",
    "            validation_data=(x_test, y_test_cat),\n",
    "            callbacks=[\n",
    "                EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "                ModelCheckpoint(f'dnn_classifier_fold{count}.model', save_best_only=True)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 评估分类器\n",
    "        print(f'---------------- Testing Fold {count} ------------------------')\n",
    "        loss, accuracy = dnn.evaluate(x_test, y_test_cat)\n",
    "        print(f'\\n Test Loss: {loss}')\n",
    "        print(f'\\n Test Accuracy: {accuracy}')\n",
    "\n",
    "        # 保存预测结果\n",
    "        predictions = dnn.predict(x_test)[:, 1]\n",
    "        true_labels = y_test_cat[:, 1]\n",
    "        roc(true_labels, predictions)\n",
    "        all_loc_pred += predictions.tolist()\n",
    "        all_loc_label += true_labels.tolist()\n",
    "\n",
    "    # 绘制整体 ROC 曲线\n",
    "    roc(all_loc_label, all_loc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a898b339-7c70-48be-8790-6d6c2f8c52ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea2b99c-29b1-4b2b-b729-2d84c433418b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
